# Optional provider: local Ollama (commented out by default)
# LLM_USE_OLLAMA=true
# LLM_MODEL=gpt-oss:20b

# To switch to a custom non-streaming endpoint, comment out Ollama above and set:
# LLM_ENDPOINT=https://your-internal-endpoint.example.com/chat

# Default: Blackbird (non/streaming and streaming). Fill key locally.
BLACKBIRD_ENDPOINT=https://api.blackbird.y13.io/api/blackbird/v1/chat/completions
BLACKBIRD_TIER=ultra
BLACKBIRD_MODEL=gpt-oss-120b
