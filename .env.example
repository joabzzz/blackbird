## Runtime configuration for LLM backends

# Option 1: Custom non-streaming endpoint (expects { messages: [{role,content}, ...] } and returns { content } or text)
# LLM_ENDPOINT="https://your.internal.endpoint/chat"

# Option 2: Local Ollama (non/streaming). Enable and optionally set model
# LLM_USE_OLLAMA=true
# LLM_MODEL="gpt-oss:20b"

# Option 3: Blackbird (non/streaming + SSE streaming). Do NOT commit the real values.
# BLACKBIRD_ENDPOINT="https://api.some.domain.io/api/v1/chat/completions"
# BLACKBIRD_API_KEY="sk-..."
# BLACKBIRD_TIER="ultra"
# BLACKBIRD_MODEL="gpt-oss-120b"

